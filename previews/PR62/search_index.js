var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"R. J. Hyndman. Computing and Graphing Highest Density Regions. The American Statistician 50, 120–126 (1996).\n\n\n\nM.-H. Chen and Q.-M. Shao. Monte Carlo Estimation of Bayesian Credible and HPD Intervals. Journal of Computational and Graphical Statistics 8, 69–92 (1999).\n\n\n\nA. Vehtari, A. Gelman and J. Gabry. Practical Bayesian Model Evaluation Using Leave-One-out Cross-Validation and WAIC. Statistics and Computing 27, 1413–1432 (2017).\n\n\n\nA. Vehtari. Cross-Validation FAQ - loo (Jul 2024). Accessed on Dec 14, 2024.\n\n\n\nS. Watanabe. Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory. Journal of Machine Learning Research 11, 3571–3594 (2010). Accessed on Dec 14, 2024.\n\n\n\nD. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. Van Der Linde. Bayesian Measures of Model Complexity and Fit. Journal of the Royal Statistical Society Series B: Statistical Methodology 64, 583–639 (2002).\n\n\n\nY. Yao, A. Vehtari, D. Simpson and A. Gelman. Using Stacking to Average Bayesian Predictive Distributions (with Discussion). Bayesian Analysis 13, 917–1007 (2018).\n\n\n\nJ. Gabry, D. Simpson, A. Vehtari, M. Betancourt and A. Gelman. Visualization in Bayesian Workflow. Journal of the Royal Statistical Society Series A: Statistics in Society 182, 389–402 (2019).\n\n\n\nA. Gelman, B. Goodrich, J. Gabry and A. Vehtari. R-Squared for Bayesian Regression Models. The American Statistician 73, 307–309 (2019).\n\n\n\nZ. I. Botev, J. F. Grotowski and D. P. Kroese. Kernel Density Estimation via Diffusion. The Annals of Statistics 38 (2010).\n\n\n\nP.-C. Bürkner, J. Gabry and A. Vehtari. Efficient Leave-One-out Cross-Validation for Bayesian Non-Factorized Normal and Student-t Models. Computational Statistics 36, 1243–1261 (2021). Accessed on Aug 25, 2025.\n\n\n\nA. Vehtari, P. Bürkner and J. Gabry. Leave-one-out cross-validation for non-factorized models - loo (Jul 2024). Accessed on Aug 25, 2025.\n\n\n\n","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"stats.md\"]","category":"page"},{"location":"api/#Summary-statistics","page":"API","title":"Summary statistics","text":"","category":"section"},{"location":"api/#PosteriorStats.SummaryStats","page":"API","title":"PosteriorStats.SummaryStats","text":"struct SummaryStats{D, V<:(AbstractVector)}\n\nA container for a column table of values computed by summarize.\n\nThis object implements the Tables and TableTraits column table interfaces. It has a custom show method.\n\nSummaryStats behaves like an OrderedDict of columns, where the columns can be accessed using either Symbols or a 1-based integer index.\n\nname::String: The name of the collection of summary statistics, used as the table title in display.\ndata::Any: The summary statistics for each parameter. It must implement the Tables interface.\nparameter_names::AbstractVector: Names of the parameters\n\nSummaryStats([name::String,] data[, parameter_names])\nSummaryStats(data[, parameter_names]; name::String=\"SummaryStats\")\n\nConstruct a SummaryStats from tabular data with optional stats name and param_names.\n\ndata must not contain a column :parameter, as this is reserved for the parameter names, which are always in the first column.\n\n\n\n\n\n","category":"type"},{"location":"api/#PosteriorStats.summarize","page":"API","title":"PosteriorStats.summarize","text":"summarize(data; kind=:all,kwargs...) -> SummaryStats\nsummarize(data, stats_funs...; kwargs...) -> SummaryStats\n\nCompute summary statistics on each param in data.\n\nArguments\n\ndata: a 3D array of real samples with shape (draws, chains, params) or another object   for which a summarize method is defined.\nstats_funs: a collection of functions that reduces a matrix with shape (draws, chains)   to a scalar or a collection of scalars. Alternatively, an item in stats_funs may be a   Pair of the form name => fun specifying the name to be used for the statistic or of   the form (name1, ...) => fun when the function returns a collection. When the function   returns a collection, the names in this latter format must be provided.\n\nKeywords\n\nvar_names: a collection specifying the names of the parameters in data. If not   provided, the names the indices of the parameter dimension in data.\nname::String: the name of the summary statistics, used as the table title in display.\nkind::Symbol: The named collection of summary statistics to be computed:\n:all: Everything in :stats and :diagnostics\n:stats: mean, std, <ci>\n:diagnostics: ess_tail, ess_bulk, rhat, mcse_mean, mcse_std\n:all_median: Everything in :stats_median and :diagnostics_median\n:stats_median: median, mad, <ci>\n:diagnostics_median: ess_median, ess_tail, rhat, mcse_median\nkwargs: additional keyword arguments passed to default_summary_stats,   including:\nci_fun=eti: The function to compute the credible interval <ci>, if any. Supported   options are eti and hdi. CI column name is   <ci_fun><100*ci_prob>.\nci_prob=0.94: The probability mass to be contained in the credible   interval <ci>.\n\nSee also SummaryStats, default_summary_stats\n\nExtended Help\n\nExamples\n\nCompute all summary statistics (the default):\n\ndetails: Display precision\nWhen an estimator and its MCSE are both computed, the MCSE is used to determine the number of significant digits that will be displayed.\n\njulia> using Statistics, StatsBase\n\njulia> x = randn(1000, 4, 3) .+ reshape(0:10:20, 1, 1, :);\n\njulia> summarize(x)\nSummaryStats\n       mean   std  eti94          ess_tail  ess_bulk  rhat  mcse_mean  mcse_std\n 1   0.0003  0.99  -1.83 .. 1.89      3567      3663  1.00      0.016     0.012\n 2  10.02    0.99   8.17 .. 11.9      3841      3906  1.00      0.016     0.011\n 3  19.98    0.99   18.1 .. 21.9      3892      3749  1.00      0.016     0.012\n\nCompute just the default statistics with an 89% HDI, and provide the parameter names:\n\njulia> var_names=[:x, :y, :z];\n\njulia> summarize(x; var_names, kind=:stats, ci_fun=hdi, ci_prob=0.89)\nSummaryStats\n         mean    std  hdi89\n x   0.000275  0.989  -1.63 .. 1.52\n y  10.0       0.988   8.53 .. 11.6\n z  20.0       0.988   18.5 .. 21.6\n\nCompute Statistics.mean, Statistics.std and the Monte Carlo standard error (MCSE) of the mean estimate:\n\njulia> summarize(x, mean, std, :mcse_mean => sem; name=\"Mean/Std\")\nMean/Std\n       mean    std  mcse_mean\n 1   0.0003  0.989      0.016\n 2  10.02    0.988      0.016\n 3  19.98    0.988      0.016\n\nCompute multiple quantiles simultaneously:\n\njulia> percs = (5, 25, 50, 75, 95);\n\njulia> summarize(x, Symbol.(:q, percs) => Base.Fix2(quantile, percs ./ 100))\nSummaryStats\n       q5     q25       q50     q75    q95\n 1  -1.61  -0.668   0.00447   0.653   1.64\n 2   8.41   9.34   10.0      10.7    11.6\n 3  18.4   19.3    20.0      20.6    21.6\n\nExtending summarize to custom types\n\nTo support computing summary statistics from a custom object MyType, overload the method summarize(::MyType, stats_funs...; kwargs...), which should ultimately call summarize(::AbstractArray{<:Union{Real,Missing},3}, stats_funs...; other_kwargs...), where other_kwargs are the keyword arguments passed to summarize.\n\n\n\n\n\n","category":"function"},{"location":"api/#PosteriorStats.default_summary_stats","page":"API","title":"PosteriorStats.default_summary_stats","text":"default_summary_stats(kind::Symbol=:all; kwargs...)\n\nReturn a collection of stats functions based on the named preset kind.\n\nThese functions are then passed to summarize.\n\nArguments\n\nkind::Symbol: The named collection of summary statistics to be computed:\n:all: Everything in :stats and :diagnostics\n:stats: mean, std, <ci>\n:diagnostics: ess_tail, ess_bulk, rhat, mcse_mean, mcse_std\n:all_median: Everything in :stats_median and :diagnostics_median\n:stats_median: median, mad, <ci>\n:diagnostics_median: ess_median, ess_tail, rhat, mcse_median\n\nKeywords\n\nci_fun=eti: The function to compute the credible interval <ci>, if any. Supported   options are eti and hdi. CI column name is   <ci_fun><100*ci_prob>.\nci_prob=0.94: The probability mass to be contained in the credible   interval <ci>.\n\n\n\n\n\n","category":"function"},{"location":"api/#Credible-intervals","page":"API","title":"Credible intervals","text":"","category":"section"},{"location":"api/#PosteriorStats.hdi","page":"API","title":"PosteriorStats.hdi","text":"hdi(samples::AbstractVecOrMat{<:Real}; [prob, sorted, method]) -> IntervalSets.ClosedInterval\nhdi(samples::AbstractArray{<:Real}; [prob, sorted, method]) -> Array{<:IntervalSets.ClosedInterval}\n\nEstimate the highest density interval (HDI) of samples for the probability prob.\n\nThe HDI is the minimum width Bayesian credible interval (BCI). That is, it is the smallest possible interval containing (100*prob)% of the probability mass.[1] This implementation uses the algorithm of Chen and Shao [2].\n\nSee also: hdi!, eti, eti!.\n\nArguments\n\nsamples: an array of shape (draws[, chains[, params...]]). If multiple parameters are   present, a marginal HDI is computed for each.\n\nKeywords\n\nprob: the probability mass to be contained in the HDI. Default is 0.94.\nsorted=false: if true, the input samples are assumed to be sorted.\nmethod::Symbol: the method used to estimate the HDI. Available options are:\n:unimodal: Assumes a unimodal distribution (default). Bounds are entries in samples.\n:multimodal: Fits a density estimator to samples and finds the HDI of the estimated   density. For continuous data, the density estimator is a kernel density estimate (KDE)   computed using kde_reflected. For discrete data, a histogram with bin width   1 is used.\n:multimodal_sample: Like :multimodal, but uses the density estimator to find the   entries in samples with the highest density and computes the HDI from those points.   This can correct for inaccuracies in the density estimator.\nis_discrete::Union{Bool,Nothing}=nothing: Specify if the data is discrete   (integer-valued). If nothing, it's automatically determined.\nkwargs: For continuous data and multimodal methods, remaining keywords are forwarded   to kde_reflected.\n\nReturns\n\nintervals: If samples is a vector or matrix, then a single   IntervalSets.ClosedInterval is returned for :unimodal method, or a vector   of ClosedInterval for multimodal methods. For higher dimensional inputs, an array with   the shape (params...,) is returned, containing marginal HDIs for each parameter.\n\nnote: Note\nAny default value of prob is arbitrary. The default value of prob=0.94 instead of a more common default like prob=0.95 is chosen to remind the user of this arbitrariness.\n\nExamples\n\nHere we calculate the 83% HDI for a normal random variable:\n\njulia> x = randn(2_000);\n\njulia> hdi(x; prob=0.83)\n-1.3826605224220527 .. 1.259817149822839\n\nWe can also calculate the HDI for a 3-dimensional array of samples:\n\njulia> x = randn(1_000, 1, 1) .+ reshape(0:5:10, 1, 1, :);\n\njulia> hdi(x)\n3-element Vector{IntervalSets.ClosedInterval{Float64}}:\n -1.6402043796029502 .. 2.041852066407182\n 3.35979562039705 .. 7.041852066407182\n 8.35979562039705 .. 12.041852066407182\n\nFor multimodal distributions, you can use the :multimodal method:\n\njulia> x = vcat(randn(1000), randn(1000) .+ 5);\n\njulia> hdi(x; method=:multimodal)\n2-element Vector{IntervalSets.ClosedInterval{Float64}}:\n -1.8882401079608677 .. 2.0017686164555037\n 2.9839268475847436 .. 6.9235952578447275\n\nReferences\n\n[1] Hyndman, J. Comput. Graph. Stat., 50:2 (1996)\n[2] Chen & Shao, J Comput. Graph. Stat., 8:1 (1999)\n\n\n\n\n\n","category":"function"},{"location":"api/#PosteriorStats.hdi!","page":"API","title":"PosteriorStats.hdi!","text":"hdi!(samples::AbstractArray{<:Real}; [prob, sorted])\n\nA version of hdi that partially sorts samples in-place while computing the HDI.\n\nSee also: hdi, eti, eti!.\n\n\n\n\n\n","category":"function"},{"location":"api/#PosteriorStats.eti","page":"API","title":"PosteriorStats.eti","text":"eti(samples::AbstractVecOrMat{<:Real}; [prob, kwargs...]) -> IntervalSets.ClosedInterval\neti(samples::AbstractArray{<:Real}; [prob, kwargs...]) -> Array{<:IntervalSets.ClosedInterval}\n\nEstimate the equal-tailed interval (ETI) of samples for the probability prob.\n\nThe ETI of a given probability is the credible interval wih the property that the probability of being below the interval is equal to the probability of being above it. That is, it is defined by the (1-prob)/2 and 1 - (1-prob)/2 quantiles of the samples.\n\nSee also: eti!, hdi, hdi!.\n\nArguments\n\nsamples: an array of shape (draws[, chains[, params...]]). If multiple parameters are   present\n\nKeywords\n\nprob: the probability mass to be contained in the ETI. Default is 0.94.\nkwargs: remaining keywords are passed to Statistics.quantile.\n\nReturns\n\nintervals: If samples is a vector or matrix, then a single   IntervalSets.ClosedInterval is returned. Otherwise, an array with the shape   (params...,), is returned, containing a marginal ETI for each parameter.\n\nnote: Note\nAny default value of prob is arbitrary. The default value of prob=0.94 instead of a more common default like prob=0.95 is chosen to reminder the user of this arbitrariness.\n\nExamples\n\nHere we calculate the 83% ETI for a normal random variable:\n\njulia> x = randn(2_000);\n\njulia> eti(x; prob=0.83)\n-1.3740585250299766 .. 1.2860771129421198\n\nWe can also calculate the ETI for a 3-dimensional array of samples:\n\njulia> x = randn(1_000, 1, 1) .+ reshape(0:5:10, 1, 1, :);\n\njulia> eti(x)\n3-element Vector{IntervalSets.ClosedInterval{Float64}}:\n -1.951006825019686 .. 1.9011666217153793\n 3.048993174980314 .. 6.9011666217153795\n 8.048993174980314 .. 11.90116662171538\n\n\n\n\n\n","category":"function"},{"location":"api/#PosteriorStats.eti!","page":"API","title":"PosteriorStats.eti!","text":"eti!(samples::AbstractArray{<:Real}; [prob, kwargs...])\n\nA version of eti that partially sorts samples in-place while computing the ETI.\n\nSee also: eti, hdi, hdi!.\n\n\n\n\n\n","category":"function"},{"location":"api/#LOO-and-WAIC","page":"API","title":"LOO and WAIC","text":"","category":"section"},{"location":"api/#PosteriorStats.AbstractELPDResult","page":"API","title":"PosteriorStats.AbstractELPDResult","text":"abstract type AbstractELPDResult\n\nAn abstract type representing the result of an ELPD computation.\n\nEvery subtype stores estimates of both the expected log predictive density (elpd) and the effective number of parameters p, as well as standard errors and pointwise estimates of each, from which other relevant estimates can be computed.\n\nSubtypes implement the following functions:\n\nelpd_estimates\ninformation_criterion\n\n\n\n\n\n","category":"type"},{"location":"api/#PosteriorStats.PSISLOOResult","page":"API","title":"PosteriorStats.PSISLOOResult","text":"Results of Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO).\n\nSee also: loo, AbstractELPDResult\n\nestimates: Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p)\npointwise: Pointwise estimates\npsis_result: A PSIS.PSISResult with Pareto-smoothed importance sampling (PSIS) results\n\n\n\n\n\n","category":"type"},{"location":"api/#PosteriorStats.WAICResult","page":"API","title":"PosteriorStats.WAICResult","text":"Results of computing the widely applicable information criterion (WAIC).\n\nSee also: waic, AbstractELPDResult\n\nestimates: Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p)\npointwise: Pointwise estimates\n\n\n\n\n\n","category":"type"},{"location":"api/#PosteriorStats.elpd_estimates","page":"API","title":"PosteriorStats.elpd_estimates","text":"elpd_estimates(result::AbstractELPDResult; pointwise=false) -> (; elpd, se_elpd, lpd)\n\nReturn the (E)LPD estimates from the result.\n\n\n\n\n\n","category":"function"},{"location":"api/#PosteriorStats.information_criterion","page":"API","title":"PosteriorStats.information_criterion","text":"information_criterion(elpd, scale::Symbol)\n\nCompute the information criterion for the given scale from the elpd estimate.\n\nscale must be one of (:deviance, :log, :negative_log).\n\nSee also: loo, waic\n\n\n\n\n\ninformation_criterion(result::AbstractELPDResult, scale::Symbol; pointwise=false)\n\nCompute information criterion for the given scale from the existing ELPD result.\n\nscale must be one of (:deviance, :log, :negative_log).\n\nIf pointwise=true, then pointwise estimates are returned.\n\n\n\n\n\n","category":"function"},{"location":"api/#PosteriorStats.loo","page":"API","title":"PosteriorStats.loo","text":"loo(log_likelihood; reff=nothing, kwargs...) -> PSISLOOResult{<:NamedTuple,<:NamedTuple}\n\nCompute the Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO). [3, 4]\n\nlog_likelihood must be an array of log-likelihood values with shape (chains, draws[, params...]).\n\nKeywords\n\nreff::Union{Real,AbstractArray{<:Real}}: The relative effective sample size(s) of the likelihood values. If an array, it must have the same data dimensions as the corresponding log-likelihood variable. If not provided, then this is estimated using MCMCDiagnosticTools.ess.\nkwargs: Remaining keywords are forwarded to PSIS.psis.\n\nSee also: PSISLOOResult, waic\n\nExamples\n\nManually compute R_mathrmeff and calculate PSIS-LOO of a model:\n\njulia> using ArviZExampleData, MCMCDiagnosticTools\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> reff = ess(log_like; kind=:basic, split_chains=1, relative=true);\n\njulia> loo(log_like; reff)\nPSISLOOResult with estimates\n elpd  se_elpd    p  se_p\n  -31      1.4  0.9  0.33\n\nand PSISResult with 500 draws, 4 chains, and 8 parameters\nPareto shape (k) diagnostic values:\n                    Count      Min. ESS\n (-Inf, 0.5]  good  5 (62.5%)  290\n  (0.5, 0.7]  okay  3 (37.5%)  399\n\nReferences\n\n[3] Vehtari et al. Stat. Comput. 27 (2017).\n[4] Vehtari. Cross-validation FAQ.\n\n\n\n\n\n","category":"function"},{"location":"api/#PosteriorStats.waic","page":"API","title":"PosteriorStats.waic","text":"waic(log_likelihood::AbstractArray) -> WAICResult{<:NamedTuple,<:NamedTuple}\n\nCompute the widely applicable information criterion (WAIC). [5]\n\nlog_likelihood must be an array of log-likelihood values with shape (chains, draws[, params...]).\n\nSee also: WAICResult, loo\n\nExamples\n\nCalculate WAIC of a model:\n\njulia> using ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> waic(log_like)\nWAICResult with estimates\n elpd  se_elpd    p  se_p\n  -31      1.4  0.9  0.32\n\nReferences\n\n[5] Watanabe, JMLR 11(116) (2010)\n\n\n\n\n\n","category":"function"},{"location":"api/#Model-comparison","page":"API","title":"Model comparison","text":"","category":"section"},{"location":"api/#PosteriorStats.ModelComparisonResult","page":"API","title":"PosteriorStats.ModelComparisonResult","text":"ModelComparisonResult\n\nResult of model comparison using ELPD.\n\nThis struct implements the Tables and TableTraits interfaces.\n\nEach field returns a collection of the corresponding entry for each model:\n\nname: Names of the models, if provided.\nrank: Ranks of the models (ordered by decreasing ELPD)\nelpd_diff: ELPD of a model subtracted from the largest ELPD of any model\nse_elpd_diff: Standard error of the ELPD difference\nweight: Model weights computed with weights_method\nelpd_result: AbstactELPDResults for each model, which can be used to access useful stats like ELPD estimates, pointwise estimates, and Pareto shape values for PSIS-LOO\nweights_method: Method used to compute model weights with model_weights\n\n\n\n\n\n","category":"type"},{"location":"api/#PosteriorStats.compare","page":"API","title":"PosteriorStats.compare","text":"compare(models; kwargs...) -> ModelComparisonResult\n\nCompare models based on their expected log pointwise predictive density (ELPD).\n\nThe ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). loo is the default and recommended method for computing the ELPD. For more theory, see Spiegelhalter et al. [6].\n\nArguments\n\nmodels: a Tuple, NamedTuple, or AbstractVector whose values are either AbstractELPDResult entries or any argument to elpd_method.\n\nKeywords\n\nweights_method::AbstractModelWeightsMethod=Stacking(): the method to be used to weight the models. See model_weights for details\nelpd_method=loo: a method that computes an AbstractELPDResult from an argument in models.\nsort::Bool=true: Whether to sort models by decreasing ELPD.\n\nReturns\n\nModelComparisonResult: A container for the model comparison results. The fields contain a similar collection to models.\n\nExamples\n\nCompare the centered and non centered models of the eight school problem using the defaults: loo and Stacking weights. A custom myloo method formates the inputs as expected by loo.\n\njulia> using ArviZExampleData\n\njulia> models = (\n           centered=load_example_data(\"centered_eight\"),\n           non_centered=load_example_data(\"non_centered_eight\"),\n       );\n\njulia> function myloo(idata)\n           log_like = PermutedDimsArray(idata.log_likelihood.obs, (2, 3, 1))\n           return loo(log_like)\n       end;\n\njulia> mc = compare(models; elpd_method=myloo)\n┌ Warning: 1 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/.julia/packages/PSIS/...\nModelComparisonResult with Stacking weights\n               rank  elpd  se_elpd  elpd_diff  se_elpd_diff  weight    p  se_p\n non_centered     1   -31      1.5       0            0.0       1.0  0.9  0.32\n centered         2   -31      1.4       0.03         0.061     0.0  0.9  0.33\njulia> mc.weight |> pairs\npairs(::NamedTuple) with 2 entries:\n  :non_centered => 1.0\n  :centered     => 3.50546e-31\n\nCompare the same models from pre-computed PSIS-LOO results and computing BootstrappedPseudoBMA weights:\n\njulia> elpd_results = mc.elpd_result;\n\njulia> compare(elpd_results; weights_method=BootstrappedPseudoBMA())\nModelComparisonResult with BootstrappedPseudoBMA weights\n               rank  elpd  se_elpd  elpd_diff  se_elpd_diff  weight    p  se_p\n non_centered     1   -31      1.5       0            0.0      0.51  0.9  0.32\n centered         2   -31      1.4       0.03         0.061    0.49  0.9  0.33\n\nReferences\n\n[6] Spiegelhalter et al. J. R. Stat. Soc. B 64 (2002)\n\n\n\n\n\n","category":"function"},{"location":"api/#PosteriorStats.model_weights","page":"API","title":"PosteriorStats.model_weights","text":"model_weights(elpd_results; method=Stacking())\nmodel_weights(method::AbstractModelWeightsMethod, elpd_results)\n\nCompute weights for each model in elpd_results using method.\n\nelpd_results is a Tuple, NamedTuple, or AbstractVector with AbstractELPDResult entries. The weights are returned in the same type of collection.\n\nStacking is the recommended approach, as it performs well even when the true data generating process is not included among the candidate models. See Yao et al. [7] for details.\n\nSee also: AbstractModelWeightsMethod, compare\n\nExamples\n\nCompute Stacking weights for two models:\n\njulia> using ArviZExampleData\n\njulia> models = (\n           centered=load_example_data(\"centered_eight\"),\n           non_centered=load_example_data(\"non_centered_eight\"),\n       );\n\njulia> elpd_results = map(models) do idata\n           log_like = PermutedDimsArray(idata.log_likelihood.obs, (2, 3, 1))\n           return loo(log_like)\n       end;\n┌ Warning: 1 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/.julia/packages/PSIS/...\n\njulia> model_weights(elpd_results; method=Stacking()) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :centered     => 3.50546e-31\n  :non_centered => 1.0\n\nNow we compute BootstrappedPseudoBMA weights for the same models:\n\njulia> model_weights(elpd_results; method=BootstrappedPseudoBMA()) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :centered     => 0.492513\n  :non_centered => 0.507487\n\nReferences\n\n[7] Yao et al. Bayesian Analysis 13, 3 (2018)\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"The following model weighting methods are available","category":"page"},{"location":"api/#PosteriorStats.AbstractModelWeightsMethod","page":"API","title":"PosteriorStats.AbstractModelWeightsMethod","text":"abstract type AbstractModelWeightsMethod\n\nAn abstract type representing methods for computing model weights.\n\nSubtypes implement model_weights(method, elpd_results).\n\n\n\n\n\n","category":"type"},{"location":"api/#PosteriorStats.BootstrappedPseudoBMA","page":"API","title":"PosteriorStats.BootstrappedPseudoBMA","text":"struct BootstrappedPseudoBMA{R<:Random.AbstractRNG, T<:Real} <: AbstractModelWeightsMethod\n\nModel weighting method using pseudo Bayesian Model Averaging using Akaike-type weighting with the Bayesian bootstrap (pseudo-BMA+)[7].\n\nThe Bayesian bootstrap stabilizes the model weights.\n\nBootstrappedPseudoBMA(; rng=Random.default_rng(), samples=1_000, alpha=1)\nBootstrappedPseudoBMA(rng, samples, alpha)\n\nConstruct the method.\n\nrng::Random.AbstractRNG: The random number generator to use for the Bayesian bootstrap\nsamples::Int64: The number of samples to draw for bootstrapping\nalpha::Real: The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. The default (1) corresponds to a uniform distribution on the simplex.\n\nSee also: Stacking\n\nReferences\n\n[7] Yao et al. Bayesian Analysis 13, 3 (2018)\n\n\n\n\n\n","category":"type"},{"location":"api/#PosteriorStats.PseudoBMA","page":"API","title":"PosteriorStats.PseudoBMA","text":"struct PseudoBMA <: AbstractModelWeightsMethod\n\nModel weighting method using pseudo Bayesian Model Averaging (pseudo-BMA) and Akaike-type weighting.\n\nPseudoBMA(; regularize=false)\nPseudoBMA(regularize)\n\nConstruct the method with optional regularization of the weights using the standard error of the ELPD estimate.\n\nnote: Note\nThis approach is not recommended, as it produces unstable weight estimates. It is recommended to instead use BootstrappedPseudoBMA to stabilize the weights or Stacking. For details, see Yao et al. [7].\n\nSee also: Stacking\n\nReferences\n\n[7] Yao et al. Bayesian Analysis 13, 3 (2018)\n\n\n\n\n\n","category":"type"},{"location":"api/#PosteriorStats.Stacking","page":"API","title":"PosteriorStats.Stacking","text":"struct Stacking{O<:Optim.AbstractOptimizer} <: AbstractModelWeightsMethod\n\nModel weighting using stacking of predictive distributions[7].\n\nStacking(; optimizer=Optim.LBFGS(), options=Optim.Options()\nStacking(optimizer[, options])\n\nConstruct the method, optionally customizing the optimization.\n\noptimizer::Optim.AbstractOptimizer: The optimizer to use for the optimization of the weights. The optimizer must support projected gradient optimization via a manifold field.\noptions::Optim.Options: The Optim options to use for the optimization of the weights.\n\nSee also: BootstrappedPseudoBMA\n\nReferences\n\n[7] Yao et al. Bayesian Analysis 13, 3 (2018)\n\n\n\n\n\n","category":"type"},{"location":"api/#Predictive-checks","page":"API","title":"Predictive checks","text":"","category":"section"},{"location":"api/#PosteriorStats.loo_pit","page":"API","title":"PosteriorStats.loo_pit","text":"loo_pit(y, y_pred, log_weights; kwargs...) -> Union{Real,AbstractArray}\n\nCompute leave-one-out probability integral transform (LOO-PIT) checks.\n\nArguments\n\ny: array of observations with shape (params...,)\ny_pred: array of posterior predictive samples with shape (draws, chains, params...).\nlog_weights: array of normalized log LOO importance weights with shape (draws, chains, params...).\n\nKeywords\n\nis_discrete: If not provided, then it is set to true iff elements of y and y_pred are all integer-valued. If true, then data are smoothed using smooth_data to make them non-discrete before estimating LOO-PIT values.\nkwargs: Remaining keywords are forwarded to smooth_data if data is discrete.\n\nReturns\n\npitvals: LOO-PIT values with same size as y. If y is a scalar, then pitvals is a scalar.\n\nLOO-PIT is a marginal posterior predictive check. If y_-i is the array y of observations with the ith observation left out, and y_i^* is a posterior prediction of the ith observation, then the LOO-PIT value for the ith observation is defined as\n\nP(y_i^* le y_i mid y_-i) = int_-infty^y_i p(y_i^* mid y_-i) mathrmd y_i^*\n\nThe LOO posterior predictions and the corresponding observations should have similar distributions, so if conditional predictive distributions are well-calibrated, then all LOO-PIT values should be approximately uniformly distributed on 0 1. [8]\n\nExamples\n\nCalculate LOO-PIT values using as test quantity the observed values themselves.\n\njulia> using ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> y = idata.observed_data.obs;\n\njulia> y_pred = PermutedDimsArray(idata.posterior_predictive.obs, (:draw, :chain, :school));\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> log_weights = loo(log_like).psis_result.log_weights;\n\njulia> loo_pit(y, y_pred, log_weights)\n┌ 8-element DimArray{Float64, 1} ┐\n├────────────────────────────────┴─────────────────────────────── dims ┐\n  ↓ school Categorical{String} [\"Choate\", …, \"Mt. Hermon\"] Unordered\n└──────────────────────────────────────────────────────────────────────┘\n \"Choate\"            0.942759\n \"Deerfield\"         0.641057\n \"Phillips Andover\"  0.32729\n \"Phillips Exeter\"   0.581451\n \"Hotchkiss\"         0.288523\n \"Lawrenceville\"     0.393741\n \"St. Paul's\"        0.886175\n \"Mt. Hermon\"        0.638821\n\nCalculate LOO-PIT values using as test quantity the square of the difference between each observation and mu.\n\njulia> using Statistics\n\njulia> mu = idata.posterior.mu;\n\njulia> T = y .- median(mu);\n\njulia> T_pred = y_pred .- mu;\n\njulia> loo_pit(T .^ 2, T_pred .^ 2, log_weights)\n┌ 8-element DimArray{Float64, 1} ┐\n├────────────────────────────────┴─────────────────────────────── dims ┐\n  ↓ school Categorical{String} [\"Choate\", …, \"Mt. Hermon\"] Unordered\n└──────────────────────────────────────────────────────────────────────┘\n \"Choate\"            0.868148\n \"Deerfield\"         0.27421\n \"Phillips Andover\"  0.321719\n \"Phillips Exeter\"   0.193169\n \"Hotchkiss\"         0.370422\n \"Lawrenceville\"     0.195601\n \"St. Paul's\"        0.817408\n \"Mt. Hermon\"        0.326795\n\nReferences\n\n[8] Gabry et al. J. R. Stat. Soc. Ser. A Stat. Soc. 182 (2019).\n\n\n\n\n\n","category":"function"},{"location":"api/#PosteriorStats.r2_score","page":"API","title":"PosteriorStats.r2_score","text":"r2_score(y_true::AbstractVector, y_pred::AbstractArray) -> (; r2, r2_std)\n\nR² for linear Bayesian regression models.[9]\n\nArguments\n\ny_true: Observed data of length noutputs\ny_pred: Predicted data with size (ndraws[, nchains], noutputs)\n\nExamples\n\njulia> using ArviZExampleData\n\njulia> idata = load_example_data(\"regression1d\");\n\njulia> y_true = idata.observed_data.y;\n\njulia> y_pred = PermutedDimsArray(idata.posterior_predictive.y, (:draw, :chain, :y_dim_0));\n\njulia> r2_score(y_true, y_pred) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :r2     => 0.683197\n  :r2_std => 0.0368838\n\nReferences\n\n[9] Gelman et al, The Am. Stat., 73(3) (2019)\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/#PosteriorStats.kde_reflected","page":"API","title":"PosteriorStats.kde_reflected","text":"kde_reflected(data::AbstractVector{<:Real}; bounds=extrema(data), kwargs...)\n\nCompute the boundary-corrected kernel density estimate (KDE) of data using reflection.\n\nFor x in (l u), the reflected KDE has the density\n\nhatf_R(x) = hatf(x) + hatf(2l - x) + hatf(2u - x)\n\nwhere hatf is the usual KDE of data. This is equivalent to augmenting the original data with 2 additional copies of the data reflected around each bound, computing the usual KDE, trimming the KDE to the bounds, and renormalizing.\n\nAny non-finite bounds are ignored. Remaining kwargs are passed to KernelDensity.kde. The default bandwidth is estimated using the Improved Sheather-Jones (ISJ) method [10].\n\nReferences\n\n[10] Botev et al. Ann. Stat., 38: 5 (2010)\n\n\n\n\n\n","category":"function"},{"location":"api/#PosteriorStats.pointwise_loglikelihoods","page":"API","title":"PosteriorStats.pointwise_loglikelihoods","text":"pointwise_loglikelihoods\n\nCompute pointwise conditional log-likelihoods for ELPD-based model comparison/validation.\n\nGiven model parameters theta and observations y, the pointwise conditional log-likelihood of y_i given y_-i (the elements of y excluding y_i) and theta is defined as\n\nlog p(y_i mid y_-i theta)\n\nThis method is a utility function that dependant packages can override to provide pointwise conditional log-likelihoods for their own models/distributions.\n\nSee also: loo\n\n\n\n\n\n","category":"function"},{"location":"api/#PosteriorStats.pointwise_loglikelihoods-Union{Tuple{N}, Tuple{M}, Tuple{AbstractArray{<:Real, N}, AbstractArray{<:Distribution{<:ArrayLikeVariate{N}}, M}}} where {M, N}","page":"API","title":"PosteriorStats.pointwise_loglikelihoods","text":"pointwise_loglikelihoods(y, dists)\n\nCompute pointwise conditional log-likelihoods of y for non-factorized distributions.\n\nA non-factorized observation model p(y mid theta), where y is an array of observations and theta are model parameters, can be factorized as p(y_i mid y_-i theta) p(y_-i mid theta). However, completely factorizing into individual likelihood terms can be tedious, expensive, and poorly supported by a given PPL. This utility function computes log p(y_i mid y_-i theta) terms for all i; the resulting pointwise conditional log-likelihoods can be used e.g. in loo.\n\nArguments\n\ny: array of observations with shape (params...,)\ndists: array of shape (draws[, chains]) containing parametrized Distributions.Distributions representing a non-factorized observation model, one for each posterior draw. The following distributions are currently supported:\nDistributions.MvNormal [11]\nDistributions.MvNormalCanon\nDistributions.MatrixNormal\nDistributions.MvLogNormal\nDistributions.GenericMvTDist [11, but uses a more efficient implementation]\n\nReturns\n\nlog_like: log-likelihood values with shape (draws[, chains], params...)\n\nReferences\n\n[11] Bürkner et al. Comput. Stat. 36 (2021).\n[12] Vehtari et al. Leave-one-out cross-validation for non-factorized   models\n\n\n\n\n\n","category":"method"},{"location":"api/#PosteriorStats.smooth_data","page":"API","title":"PosteriorStats.smooth_data","text":"smooth_data(y; dims=:, interp_method=CubicSpline, offset_frac=0.01)\n\nSmooth y along dims using interp_method.\n\ninterp_method is a 2-argument callabale that takes the arguments y and x and returns a DataInterpolations.jl interpolation method, defaulting to a cubic spline interpolator.\n\noffset_frac is the fraction of the length of y to use as an offset when interpolating.\n\n\n\n\n\n","category":"function"},{"location":"#PosteriorStats","page":"Home","title":"PosteriorStats","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"PosteriorStats implements widely-used and well-characterized statistical analyses for the Bayesian workflow. These functions generally estimate properties of posterior and/or posterior predictive distributions. The default implementations defined here operate on Monte Carlo samples.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See the API for details.","category":"page"},{"location":"#Extending-this-package","page":"Home","title":"Extending this package","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The methods defined here are intended to be extended by two types of packages.","category":"page"},{"location":"","page":"Home","title":"Home","text":"packages that implement data types for storing Monte Carlo samples\npackages that implement other representations for posterior distributions than Monte Carlo draws","category":"page"}]
}
